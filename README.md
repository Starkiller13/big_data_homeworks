# Bid Data Homeworks (unipd 2020/2021)

Made by Andrea Costalonga, Elena Bettella, Amanpreet Singh

# Prerequisites
You need to have java and spark installed on your machine.
Pyspark is also needed
```bash
pip install pyspark
```

# Homework 1: User Ratings
Basis of Map Reduce paradigm, our goal was to fetch and make some 
operations over a online shopping dataset of reviews.
Original instructions can be found [here](https://github.com/Starkiller13/big_data_homeworks/blob/final/big_data_hw1/hw1_description)

Usage(starting from the main directory)
```bash
cd /big_data_hw1
python G33HW1.py <K> <T> <file_name>
```

Mark: 7.0/7.0

# Homework 2: Clustering
Starting from an already clustered pointset we had to compute an 
approximate silhouette coefficient for the entire pointset and 
the exact silhouette coefficient for a sample obtained using poisson sampling.
Original instructions can be found [here](https://github.com/Starkiller13/big_data_homeworks/blob/final/big_data_hw2/hw2_description)

Usage(starting from the main directory)
```bash
cd /big_data_hw2
# k number of clusters in which the points are classified, 
# t parameter used for the poisson sampling
python G33HW2.py <file_name> <k> <t>
```

Mark: TBD

# Homework 3: TBD
