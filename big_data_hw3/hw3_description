Assignment of Homework 3: DEADLINE June 13, 23.59pm
The purpose of this homework is to run a Spark program on the CloudVeneto cluster available for the course. The objective of the program is the selection of a suitable number of clusters for a given input dataset using the silhouette coefficient. The program must do so by pipelining the Spark implementation of Lloyd's algorithm and the approximation of the silhouette developed for Homework 2. In the homework you will test the scalability of the various steps of the pipeline.

Using CloudVeneto
A brief description of the cluster available for the course, together with instructions on how to access the cluster and how to run your program on it are given in this User guide for the cluster on CloudVeneto.

Spark implementation of Lloyd's algorithm.
In the RDD-based API of the mllib package, Spark provides an implementation of LLoyd's algorithm for k-means clustering. In particular, the algorithm is implemented by method train of class KMeans which receives in input the points stored as an RDD of Vector, in Java, and of NumPy arrays in Python, the number k of clusters, and the number of iterations. The method computes an initial set of centers using, as a default, algorithm kmeans|| (a parallel variant of kmeans++), and then executes the specified number of iterations. As output the method returns the final set of centers, represented as an instance of class KMeansModel. For this latter class, method clusterCenters will return the centers as an array of Vector (in Java) or list of NumPy arrays in Python. Refer to the official Spark documentation on clustering (RDD-based API) for more details.

Assignment
You must write a program GxxHW3.java (for Java users) or GxxHW3.py (for Python users), where xx is your two-digit group number, which receives in input, as command-line arguments, the following data (in this ordering)
      A path to a text file containing a point set in Euclidean space. Each line of the file contains the coordinates of a point separated by spaces. (Your program should make no assumptions on the number of dimensions!)
      An integer kstart which is the initial number of clusters.
      An integer h which is the number of values of k that the program will test.   
      An integer iter which is the number of iterations of Lloyd's algorithm.
      An integer M which is the expected size of the sample used to approximate the silhouette coefficient.
      An integer L which is the number of partitions of the RDDs containing the input points and their clustering.

The program must do the following (recycle pieces of code from Homework 2 where appropriate):
      Reads the various parameters passed as command-line arguments. In particular, the set of points must be stored into an RDD called inputPoints, which must be cached and subdivided into L partitions. Two notices: (a) in the input file the coordinates of the points are separated by spaces and not by commas as in Homework 2, so take this into account where adapting the reading method used in Homework 2; (b) the textfile method invoked from the Spark context, which you will use to read the input textfile into an RDD, is able to read gzipped files as well.  After reading the parameters print the time spent to read the input points.
      For every k between kstart and kstart+h-1 does the following
      Computes a clustering of the input points with k clusters, using the Spark implementation of Lloyd's algorithm described above with iter iterations. The clustering must be stored into an RDD currentClustering of pairs (point, cluster_index) with as many elements as the input points. The RDD must be cached and partitioned into L partitions. (If computed by transforming each element of inputPoints with a map method, it should inherit its partitioning.) 
      Computes the approximate average silhouette coefficient of the clustering stored in the RDD currentClustering using the approximation algorithm implemented and tested in Homework 2, with t=M/k. In particular, the approximate silhouette coefficient of each point must be computed using a sample obtained by selecting min{t,|C|} points from each cluster C, in expectation.
      Prints the following values: (a) the value k; (b) the value of the approximate average silhouette coefficient; (c) the time spent to compute the clustering; (d) the time spent to compute the silhouette (which must include the time to extract the sample).Times must be in ms.

IMPORTANT: To define the Spark configuration in your program, use the following instructions:

(Java):
SparkConf conf = new SparkConf(true)
      .setAppName("Homework3")
      .set("spark.locality.wait", "0s")
(Python):
conf = (SparkConf().setAppName('Homework3').set('spark.locality.wait','0s'))
(The option spark.locality.wait must be set as indicated to avoid that for medium size datasets Spark uses less than the specified number of executors.) Also, do not set the master (setMaster option). This option is preconfigured on CloudVeneto.

Test your program in local mode on your PC to make sure that it runs correctly. For this local test you can use this dataset. For a description of the datasets used in the homework refer to this page.

Test your program on the cluster using the datasets which have been preloaded in the cluster. Use various configurations of parameters and report your results using the the table given in this word file.

WHEN USING THE CLUSTER, YOU MUST STRICTLY FOLLOW THESE RULES:

To avoid congestion, groups with even (resp., odd) group number must use the clusters in even (resp., odd) days.
    Do not run several instances of your program at once.
    Do not use more than 16 executors.
    Try your program on a smaller dataset first. 
    Remember that if your program is stuck for more than 1 hour, its execution will be automatically stopped by the system.
SUBMISSION INSTRUCTIONS. Each group must submit a zipped folder GxxHW3.zip, where xx is your ID group. The folder must contain the program (GxxHW3.java or GxxHW3.py) and a file GxxHW3table.docx with containing the aforementioned table. Only one student per group must do the submission using the link provided in the Homework3 section. Make sure that your code is free from compiling/run-time errors and that you comply with the specification, otherwise your score will be penalized.

If you have questions about the assignment, contact the teaching assistants (TAs) by email to bdc-course@dei.unipd.it . The subject of the email must be "HW3 - Group xx", where xx is your ID group. If needed, a zoom meeting between the TAs and the group will be organized.

